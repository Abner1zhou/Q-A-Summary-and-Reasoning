{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8 \n",
    "\"\"\"\n",
    "@author: 周世聪\n",
    "@contact: abner1zhou@gmail.com \n",
    "@file: data_processing.py \n",
    "@time: 2019/12/4 下午9:23 \n",
    "@desc: 数据处理函数，包括分词，词向量构建，embedding_matrix，训练集 测试集创建\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import jieba\n",
    "import re\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils import multi_cpus\n",
    "from utils import config\n",
    "\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    \"\"\"\n",
    "    特殊符号去除\n",
    "    使用正则表达式去除无用的符号、词语\n",
    "    \"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        return re.sub(\n",
    "            r'[\\s+\\-\\|\\!\\/\\[\\]\\{\\}_,.$%^*(+\\\"\\')]+|[:：+——()?【】“”！，。？、~@#￥%……&*（）]+|车主说|技师说|语音|图片|你好|您好|nan',\n",
    "            '', sentence)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def get_stop_words(stop_words_path):\n",
    "    \"\"\"\n",
    "    处理停用词表\n",
    "    :param stop_words_path: 停用词表路径\n",
    "    :return: 停用词表list\n",
    "    \"\"\"\n",
    "    stop = []\n",
    "    with open(stop_words_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            stop.append(line.strip())\n",
    "    return stop\n",
    "\n",
    "\n",
    "# cut函数，分别对question，dialogue，report进行切词\n",
    "def cut_words(sentences):\n",
    "    # 清除无用词\n",
    "    sentence = clean_sentence(sentences)\n",
    "    # 切词，默认精确模式，全模式cut参数cut_all=True\n",
    "    words = jieba.cut(sentence)\n",
    "    # 过滤停用词\n",
    "    stop_words = get_stop_words(config.stop_word_path)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def cut_data_frame(df):\n",
    "    \"\"\"\n",
    "    数据集批量处理方法\n",
    "    :param df: 数据集\n",
    "    :return:处理好的数据集\n",
    "    \"\"\"\n",
    "    # 批量预处理 训练集和测试集\n",
    "    for col_name in ['Brand', 'Model', 'Question', 'Dialogue']:\n",
    "        df[col_name] = df[col_name].apply(cut_words)\n",
    "\n",
    "    if 'Report' in df.columns:\n",
    "        # 训练集 Report 预处理\n",
    "        df['Report'] = df['Report'].apply(cut_words)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_segment(data_path):\n",
    "    \"\"\"\n",
    "    将输入的数据集切词并返回\n",
    "    :param data_path: 输入csv格式数据集\n",
    "    :return: 返回一个切词完毕的数据集\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_path)\n",
    "    df = df.dropna()\n",
    "    # 1.切词\n",
    "    seg_df = multi_cpus.parallelize(df, cut_data_frame)\n",
    "    # 对切词完的数据进行拼接,这部分用来训练词向量\n",
    "    seg_df[\"Data_X\"] = seg_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "    if \"Report\" in seg_df.columns:\n",
    "        seg_df['merged'] = seg_df[['Question', 'Dialogue', 'Report']].apply(lambda x: ' '.join(x), axis=1)  # axis 横向拼接\n",
    "        seg_df['Data_Y'] = seg_df[['Report']]\n",
    "    else:\n",
    "        seg_df['merged'] = seg_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)  # axis 横向拼接\n",
    "\n",
    "    return seg_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对原始数据集进行切词、删除停用词、无用字符等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_df = get_segment(config.train_data_path)\n",
    "test_seg_df = get_segment(config.test_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 合并数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([train_seg_df[['merged']], test_seg_df[['merged']]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(config.merger_seg_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v(merger_seg_path):\n",
    "    w2v_model = word2vec.Word2Vec(LineSentence(merger_seg_path), \n",
    "                            workers=6, \n",
    "                            min_count=5, # 忽略词频小于5的单词\n",
    "                            size=200)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 计算词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = get_w2v(config.merger_seg_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 获得训练集、测试集数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(data):\n",
    "    \"\"\"\n",
    "    获得合适的最大长度值\n",
    "    :param data: 待统计的数据  train_df['Question']\n",
    "    :return: 最大长度值\n",
    "    \"\"\"\n",
    "    # TODO FIX len size bug\n",
    "    max_lens = data.apply(lambda x: x.count(' ') + 1)\n",
    "    return int(np.mean(max_lens) + 2 * np.std(max_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_proc(sentence, max_len, vocab):\n",
    "    '''\n",
    "    # 填充字段\n",
    "    < start > < end > < pad > < unk > max_lens\n",
    "    '''\n",
    "    # 0.按空格统计切分出词\n",
    "    words = sentence.strip().split(' ')\n",
    "    # 1. 截取规定长度的词数\n",
    "    words = words[:max_len]\n",
    "    # 2. 填充< unk > ,判断是否在vocab中, 不在填充 < unk >\n",
    "    sentence = [word if word in vocab else '<UNK>' for word in words]\n",
    "    # 3. 填充< start > < end >\n",
    "    sentence = ['<START>'] + sentence + ['<STOP>']\n",
    "    # 4. 判断长度，填充　< pad >\n",
    "    sentence = sentence + ['<PAD>'] * (max_len - len(words))\n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_df['X'] = train_seg_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "test_seg_df['X'] = test_seg_df[['Question', 'Dialogue']].apply(lambda x: ' '.join(x), axis=1)\n",
    "train_seg_df['Y'] = train_seg_df[['Report']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_max_len = get_max_len(train_seg_df['X'])\n",
    "train_y_max_len = get_max_len(train_seg_df['Y'])\n",
    "test_x_max_len = get_max_len(test_seg_df['X'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 对训练集进行转换\n",
    "\n",
    "因为训练词向量的时候忽略了词频小于5的单词，所以需要额外的填充字符\n",
    "\n",
    "并且encoder的单元数是固定的，每段输入数据的长度需要设定为统一。多的忽略，少的填充字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_df['X'] = train_seg_df['X'].apply(lambda x: pad_proc(x, train_x_max_len, vocab))\n",
    "train_seg_df['Y'] = train_seg_df['Y'].apply(lambda x: pad_proc(x, train_x_max_len, vocab))\n",
    "test_seg_df['X'] = test_seg_df['X'].apply(lambda x: pad_proc(x, train_x_max_len, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seg_df['X'].to_csv(config.train_x_path, header=False, index=False)\n",
    "train_seg_df['Y'].to_csv(config.train_y_path, header=False, index=False)\n",
    "test_seg_df['X'].to_csv(config.test_x_path, header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 再次训练词向量，把刚才的填充字段加上去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_train_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start retrain w2v model\n",
      "1/3\n",
      "2/3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17254508, 50966850)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update word vector\n",
    "print('start retrain w2v model')\n",
    "w2v_model.build_vocab(LineSentence(config.train_x_path), update=True)\n",
    "w2v_model.train(LineSentence(config.train_x_path), epochs=wv_train_epochs, total_examples=w2v_model.corpus_count)\n",
    "print('1/3')\n",
    "w2v_model.build_vocab(LineSentence(config.train_y_path), update=True)\n",
    "w2v_model.train(LineSentence(config.train_y_path), epochs=wv_train_epochs, total_examples=w2v_model.corpus_count)\n",
    "print('2/3')\n",
    "w2v_model.build_vocab(LineSentence(config.test_x_path), update=True)\n",
    "w2v_model.train(LineSentence(config.test_x_path), epochs=wv_train_epochs, total_examples=w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(config.w2v_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 构建词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word : index for index, word in enumerate(w2v_model.wv.index2word)}\n",
    "reverse_vocab = {index : word for index, word in enumerate(w2v_model.wv.index2word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终train_X 需要转换成index， 把数据集里面的单词用index来表示出来，这样可以在encoder的时候index 得到word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_data(sentence, vocab):\n",
    "    words = sentence.split(' ')\n",
    "    ids = [vocab[word] if word in vocab else vocab['<UNK>'] for word in words]\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ids = train_seg_df['X'].apply(lambda x: translate_data(x, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [33060, 33061, 33061, 398, 985, 244, 229, 398,...\n",
       "1        [33060, 33061, 33061, 770, 33061, 382, 215, 35...\n",
       "2        [33060, 33061, 33061, 1480, 97, 461, 33061, 68...\n",
       "3        [33060, 33061, 33061, 14428, 9, 303, 61, 526, ...\n",
       "4        [33060, 33061, 33061, 1391, 97, 770, 12661, 33...\n",
       "                               ...                        \n",
       "82938    [33060, 33061, 33061, 457, 208, 326, 198, 326,...\n",
       "82939    [33060, 33061, 33061, 1251, 97, 1190, 5231, 65...\n",
       "82940    [33060, 33061, 33061, 211, 976, 50, 160, 334, ...\n",
       "82941    [33060, 33061, 33061, 13128, 3123, 4064, 25277...\n",
       "82942    [33060, 33061, 33061, 2820, 68, 1041, 1690, 18...\n",
       "Name: X, Length: 81572, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function str.upper()>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"wv_train_epochs\".upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"wv_train_epochs\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WV_TRAIN_EPOCHS'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method upper of str object at 0x7ff65f6cc430>\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_Pro_01",
   "language": "python",
   "name": "nlp_pro_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
